# Data Model: Module 4 - Vision-Language-Action (VLA)

**Date**: 2025-12-16 | **Spec**: [specs/4-vision-language-action/spec.md](specs/4-vision-language-action/spec.md) | **Plan**: [specs/4-vision-language-action/plan.md](specs/4-vision-language-action/plan.md)

## Overview

This document defines the conceptual data model for Module 4: Vision-Language-Action (VLA). Since this module focuses on educational content about vision-language-action systems in robotics, the "data model" represents the key concepts, entities, and relationships that students need to understand. This is not a traditional database schema but rather a conceptual model of the domain.

## Key Entities

### 1. Voice Command (VC)
**Description**: The spoken input from a human user that initiates a robotic action
**Attributes**:
- `command_text`: String - The transcribed text from speech
- `confidence_score`: Float - Confidence level of transcription (0.0-1.0)
- `timestamp`: DateTime - When the command was received
- `command_type`: Enum - Type classification (navigation, manipulation, information, etc.)

**Relationships**:
- One Voice Command → One Action Sequence
- One Voice Command → One User Intent

### 2. Speech Recognition Result (SRR)
**Description**: The output from the speech recognition system (e.g., OpenAI Whisper)
**Attributes**:
- `transcription`: String - The recognized text
- `language_code`: String - Detected language
- `processing_time`: Float - Time taken for recognition
- `segments`: Array - Detailed segment information

**Relationships**:
- One Speech Recognition Result ← One Voice Command
- One Speech Recognition Result → One Natural Language Input

### 3. Natural Language Input (NLI)
**Description**: The processed form of the transcribed command ready for LLM processing
**Attributes**:
- `raw_text`: String - Original transcribed text
- `processed_text`: String - Cleaned and normalized text
- `intent_classification`: String - Classified intent category
- `entities`: Object - Extracted entities (objects, locations, actions)

**Relationships**:
- One Natural Language Input ← One Speech Recognition Result
- One Natural Language Input → One Cognitive Plan

### 4. Cognitive Plan (CP)
**Description**: The high-level plan generated by the LLM from natural language input
**Attributes**:
- `plan_id`: String - Unique identifier for the plan
- `generated_by`: String - LLM model identifier
- `plan_steps`: Array - High-level steps in the plan
- `estimated_complexity`: Integer - Complexity rating (1-10)
- `required_capabilities`: Array - Robot capabilities needed

**Relationships**:
- One Cognitive Plan ← One Natural Language Input
- One Cognitive Plan → One Action Sequence
- One Cognitive Plan → Many Plan Steps

### 5. Action Sequence (AS)
**Description**: The ordered sequence of executable actions derived from the cognitive plan
**Attributes**:
- `sequence_id`: String - Unique identifier for the sequence
- `actions`: Array - Ordered list of actions to execute
- `dependencies`: Object - Dependencies between actions
- `estimated_duration`: Float - Estimated execution time
- `success_criteria`: Object - Conditions for successful completion

**Relationships**:
- One Action Sequence ← One Cognitive Plan
- One Action Sequence → Many Robot Actions
- One Action Sequence → One Execution Session

### 6. Robot Action (RA)
**Description**: A single executable action in the robot's action space
**Attributes**:
- `action_type`: String - Type of action (move_to, grasp, speak, etc.)
- `parameters`: Object - Parameters for the action
- `priority`: Integer - Execution priority (1-5)
- `preconditions`: Array - Conditions that must be met
- `postconditions`: Array - Expected state after execution

**Relationships**:
- One Robot Action ← One Action Sequence
- One Robot Action → One ROS 2 Action

### 7. ROS 2 Action (R2A)
**Description**: The specific ROS 2 action that implements the robot action
**Attributes**:
- `action_name`: String - Name of the ROS 2 action
- `action_server`: String - Name of the action server
- `request_type`: String - Type of request message
- `result_type`: String - Type of result message
- `feedback_type`: String - Type of feedback message

**Relationships**:
- One ROS 2 Action ← One Robot Action
- One ROS 2 Action → One Execution Result

### 8. Execution Result (ER)
**Description**: The outcome of executing a robot action
**Attributes**:
- `status`: String - Execution status (success, failure, partial)
- `duration`: Float - Actual execution time
- `error_message`: String - Error details if any
- `feedback_data`: Object - Runtime feedback
- `completion_percentage`: Float - How much was completed

**Relationships**:
- One Execution Result ← One ROS 2 Action
- One Execution Result → One Action Sequence Result

### 9. Action Sequence Result (ASR)
**Description**: The aggregated result of executing an entire action sequence
**Attributes**:
- `overall_status`: String - Overall success/failure
- `completed_actions`: Integer - Number of actions completed
- `failed_actions`: Integer - Number of actions failed
- `execution_time`: Float - Total execution time
- `deviation_from_plan`: Object - Differences from expected plan

**Relationships**:
- One Action Sequence Result ← One Action Sequence
- One Action Sequence Result → One User Feedback

### 10. User Feedback (UF)
**Description**: Information provided back to the user about command execution
**Attributes**:
- `feedback_type`: String - Type (success, error, progress, etc.)
- `message`: String - Human-readable message
- `timestamp`: DateTime - When feedback was generated
- `rating_request`: Boolean - Whether to request user rating

**Relationships**:
- One User Feedback ← One Action Sequence Result
- One User Feedback ← One Voice Command

## Relationships Summary

```
Voice Command (1) → Speech Recognition Result (1) → Natural Language Input (1) → Cognitive Plan (1) → Action Sequence (1) → Robot Actions (Many) → ROS 2 Actions (Many) → Execution Results (Many) → Action Sequence Result (1) → User Feedback (1)
```

Additional relationships:
- Natural Language Input ↔ Intent Classification
- Cognitive Plan ↔ Required Capabilities
- Action Sequence ↔ Dependencies
- ROS 2 Action ↔ Action Server
- Execution Result ↔ Feedback Data

## Domain Constraints

1. **Voice Command Constraint**: Each voice command must result in exactly one action sequence
2. **Cognitive Planning Constraint**: Each natural language input must be processed by an LLM to generate a cognitive plan
3. **Action Sequencing Constraint**: Actions within a sequence must respect dependencies and preconditions
4. **Execution Constraint**: Each robot action must map to a corresponding ROS 2 action
5. **Feedback Constraint**: Each action sequence execution must generate user feedback

## Educational Focus Points

For the educational content in Module 4, these data model concepts will help students understand:

1. The flow from voice input to robot action execution
2. How different system components interact
3. The role of LLMs in cognitive planning
4. The importance of action sequencing and dependencies
5. How feedback mechanisms work in VLA systems